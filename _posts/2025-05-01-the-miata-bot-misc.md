---
layout: post
title: "Miata-bot - Misc. "
excerpt_separator: <!--more-->
tags:
  - generative-ai
  - AI-ML
  - chatbot
  - fun
  - miata
  - training
last_modified_at: 2024-07-06T15:00:00-05:00
published: False
---

### Chat completion

[With llama3](https://github.com/meta-llama/llama3/blob/main/example_chat_completion.py)

### Fine tuning

[TorchTune](https://pytorch.org/torchtune/main/tutorials/e2e_flow.html)

### Evaluation

[EleutherAI - eval harness](https://github.com/EleutherAI/lm-evaluation-harness)
[Reproducible LLM Metrics](https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference)

[Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)

### General

This [OpenAI cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb?ref=blog.langchain.dev) has a useful analogy: When you fine-tune a model, it's like studying for an exam one week away. When you insert knowledge into the prompt (e.g., via retrieval), it's like taking an exam with open notes.

Fine-tuning is about improving the form of LLMs not facts [link](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts?ref=blog.langchain.dev) 